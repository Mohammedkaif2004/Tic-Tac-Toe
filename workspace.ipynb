{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b8a486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdkaif/miniforge3/envs/mdkaif/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2865a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [12:37<00:00, 252.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.35s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333669a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Classifying domains...\n"
     ]
    }
   ],
   "source": [
    "PROMPTS = {\n",
    "    \"classifier\": \"\"\"You are a medical triage classifier.\n",
    "Given the patient's question, identify the 3 most relevant medical domains.\n",
    "Return output as a JSON list: [\"domain1\",\"domain2\",\"domain3\"].\n",
    "\n",
    "Question: {q}\n",
    "\"\"\",\n",
    "    \"specialist\": \"\"\"You are a {domain} specialist doctor.\n",
    "Analyse the question carefully and answer in 3 sections:\n",
    "1) Key causes or explanations related to {domain}\n",
    "2) Recommended steps or lifestyle tips\n",
    "3) When to seek urgent medical help\n",
    "\n",
    "Question: {q}\n",
    "\"\"\",\n",
    "    \"aggregator\": \"\"\"You are the lead physician synthesizing 5 specialist reports.\n",
    "Specialist outputs:\n",
    "{specialist_outputs}\n",
    "\n",
    "Create a coherent final recommendation:\n",
    "1) Main diagnosis possibilities\n",
    "2) Common advice agreed by specialists\n",
    "3) Contradictions and resolution\n",
    "4) Final next step for patient\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Text generation helper\n",
    "# -------------------------\n",
    "def generate_text(prompt, max_new_tokens=300, temperature=0.3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# -------------------------\n",
    "# Async wrappers\n",
    "# -------------------------\n",
    "async def gen_text_async(prompt, max_new_tokens=300):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, lambda: generate_text(prompt, max_new_tokens=max_new_tokens))\n",
    "\n",
    "# -------------------------\n",
    "# Stage 1: Classifier\n",
    "# -------------------------\n",
    "async def classify_domains(query):\n",
    "    out = await gen_text_async(PROMPTS[\"classifier\"].format(q=query), max_new_tokens=200)\n",
    "    try:\n",
    "        match = re.search(r'\\[.*\\]', out, re.S)\n",
    "        domains = json.loads(match.group(0)) if match else [\"General Medicine\"]\n",
    "    except Exception:\n",
    "        domains = [\"General Medicine\"]\n",
    "    return domains[:5]\n",
    "\n",
    "# -------------------------\n",
    "# Stage 2: Specialists\n",
    "# -------------------------\n",
    "async def run_specialists(query, domains):\n",
    "    tasks = []\n",
    "    for d in domains:\n",
    "        prompt = PROMPTS[\"specialist\"].format(domain=d, q=query)\n",
    "        tasks.append(gen_text_async(prompt, max_new_tokens=250))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [{\"domain\": d, \"response\": r} for d, r in zip(domains, results)]\n",
    "\n",
    "# -------------------------\n",
    "# Stage 3: Aggregator\n",
    "# -------------------------\n",
    "async def aggregate(query, specialists):\n",
    "    joined = \"\\n\\n\".join([f\"{s['domain']} Specialist:\\n{s['response']}\" for s in specialists])\n",
    "    prompt = PROMPTS[\"aggregator\"].format(specialist_outputs=joined)\n",
    "    out = await gen_text_async(prompt, max_new_tokens=350)\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Full pipeline\n",
    "# -------------------------\n",
    "async def pipeline_run(query):\n",
    "    print(\"[Step 1] Classifying domains...\")\n",
    "    domains = await classify_domains(query)\n",
    "    print(\"Domains identified:\", domains)\n",
    "\n",
    "    print(\"\\n[Step 2] Running specialists...\")\n",
    "    specialists = await run_specialists(query, domains)\n",
    "    for s in specialists:\n",
    "        print(f\"{s['domain']} response generated.\")\n",
    "\n",
    "    print(\"\\n[Step 3] Aggregating final summary...\")\n",
    "    final = await aggregate(query, specialists)\n",
    "    return {\"domains\": domains, \"specialists\": specialists, \"final\": final}\n",
    "\n",
    "# -------------------------\n",
    "# Test run\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your medical query: \")\n",
    "\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    result = await pipeline_run(query)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"✅ FINAL SUMMARY:\")\n",
    "    print(\"==============================\\n\")\n",
    "    print(result[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my headache is happending from 8 hours and this make daily routing what shoudl it be "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdkaif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
